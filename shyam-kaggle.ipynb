{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9662712,"sourceType":"datasetVersion","datasetId":5903767}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Imports and Setup\n\nimport os\n\nimport zipfile\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torchvision import transforms, models\n\nfrom PIL import Image\n\nimport cv2\n\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:11:45.940545Z","iopub.execute_input":"2024-10-18T22:11:45.941341Z","iopub.status.idle":"2024-10-18T22:11:45.946680Z","shell.execute_reply.started":"2024-10-18T22:11:45.941295Z","shell.execute_reply":"2024-10-18T22:11:45.945662Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Cell 2: Unzip the dataset\n\n\n\nextract_path = '/kaggle/input/fruits/fruits'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:11:50.386892Z","iopub.execute_input":"2024-10-18T22:11:50.387269Z","iopub.status.idle":"2024-10-18T22:11:50.391799Z","shell.execute_reply.started":"2024-10-18T22:11:50.387235Z","shell.execute_reply":"2024-10-18T22:11:50.390702Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"classes = ['FreshApple', 'FreshBanana', 'FreshGrape', 'FreshGuava', 'FreshJujube', 'FreshOrange', 'FreshPomegranate', 'FreshStrawberry', \n\n           'RottenApple', 'RottenBanana', 'RottenGrape', 'RottenGuava', 'RottenJujube', 'RottenOrange', 'RottenPomegranate', 'RottenStrawberry']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:11:55.912346Z","iopub.execute_input":"2024-10-18T22:11:55.912748Z","iopub.status.idle":"2024-10-18T22:11:55.917631Z","shell.execute_reply.started":"2024-10-18T22:11:55.912687Z","shell.execute_reply":"2024-10-18T22:11:55.916609Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class FruitDataset(Dataset):\n\n    def __init__(self, root_dir, transform=None):\n\n        self.root_dir = root_dir\n\n        self.transform = transform\n\n        self.classes = os.listdir(root_dir)\n\n        self.file_list = []\n\n        for class_name in self.classes:\n\n            class_path = os.path.join(root_dir, class_name)\n\n            self.file_list.extend([(os.path.join(class_path, f), class_name) for f in os.listdir(class_path)])\n\n\n\n    def __len__(self):\n\n        return len(self.file_list)\n\n\n\n    def __getitem__(self, idx):\n\n        img_path, class_name = self.file_list[idx]\n\n        image = Image.open(img_path).convert('RGB')\n\n        if self.transform:\n\n            image = self.transform(image)\n\n        label = classes.index(class_name)\n\n        return image, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:12:02.462941Z","iopub.execute_input":"2024-10-18T22:12:02.463325Z","iopub.status.idle":"2024-10-18T22:12:02.471683Z","shell.execute_reply.started":"2024-10-18T22:12:02.463288Z","shell.execute_reply":"2024-10-18T22:12:02.470868Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Cell 5: Define transforms and create dataset\n\ntransform = transforms.Compose([\n\n    transforms.Resize(256),\n\n    transforms.CenterCrop(224),\n\n    transforms.ToTensor(),\n\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n\n])\n\n\n\ndataset = FruitDataset(extract_path, transform=transform)\n\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:12:06.557812Z","iopub.execute_input":"2024-10-18T22:12:06.558191Z","iopub.status.idle":"2024-10-18T22:12:08.410039Z","shell.execute_reply.started":"2024-10-18T22:12:06.558156Z","shell.execute_reply":"2024-10-18T22:12:08.409215Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Cell 6: Load and modify pre-trained ResNet model\n\nmodel = models.resnet50(pretrained=True)\n\nnum_ftrs = model.fc.in_features\n\nmodel.fc = nn.Linear(num_ftrs, len(classes))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:12:12.768604Z","iopub.execute_input":"2024-10-18T22:12:12.768987Z","iopub.status.idle":"2024-10-18T22:12:14.083956Z","shell.execute_reply.started":"2024-10-18T22:12:12.768949Z","shell.execute_reply":"2024-10-18T22:12:14.083118Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 157MB/s] \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Cell 7: Define loss function and optimizer\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:12:35.513470Z","iopub.execute_input":"2024-10-18T22:12:35.514139Z","iopub.status.idle":"2024-10-18T22:12:35.519874Z","shell.execute_reply.started":"2024-10-18T22:12:35.514098Z","shell.execute_reply":"2024-10-18T22:12:35.518884Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(torch.cuda.is_available())  # Returns True if CUDA is available, False otherwise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:12:39.300032Z","iopub.execute_input":"2024-10-18T22:12:39.300422Z","iopub.status.idle":"2024-10-18T22:12:39.362232Z","shell.execute_reply.started":"2024-10-18T22:12:39.300383Z","shell.execute_reply":"2024-10-18T22:12:39.361185Z"}},"outputs":[{"name":"stdout","text":"True\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Cell 8: Train the model\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.to(device)\n\n\n\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n\n    model.train()\n\n    running_loss = 0.0\n\n    for inputs, labels in dataloader:\n\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:12:42.905442Z","iopub.execute_input":"2024-10-18T22:12:42.906344Z","iopub.status.idle":"2024-10-18T22:32:46.417443Z","shell.execute_reply.started":"2024-10-18T22:12:42.906295Z","shell.execute_reply":"2024-10-18T22:32:46.416267Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10, Loss: 0.4707135884911594\nEpoch 2/10, Loss: 0.170788095226521\nEpoch 3/10, Loss: 0.13976493171522383\nEpoch 4/10, Loss: 0.09873821020806135\nEpoch 5/10, Loss: 0.09023547910682862\nEpoch 6/10, Loss: 0.0663026663193653\nEpoch 7/10, Loss: 0.08383496877940316\nEpoch 8/10, Loss: 0.03920772675725805\nEpoch 9/10, Loss: 0.05357802617834118\nEpoch 10/10, Loss: 0.03814680188675861\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Cell 9: Save the model\n\ntorch.save(model.state_dict(), 'fruit_freshness_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:35:08.036190Z","iopub.execute_input":"2024-10-18T22:35:08.037003Z","iopub.status.idle":"2024-10-18T22:35:08.269183Z","shell.execute_reply.started":"2024-10-18T22:35:08.036959Z","shell.execute_reply":"2024-10-18T22:35:08.268298Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Cell 10: Define prediction function\n\ndef predict(image):\n\n    img = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n\n    img_t = transform(img)\n\n    batch_t = torch.unsqueeze(img_t, 0).to(device)\n\n\n\n    model.eval()\n\n    with torch.no_grad():\n\n        out = model(batch_t)\n\n\n\n    probabilities = torch.nn.functional.softmax(out[0], dim=0)\n\n    predicted_class_index = probabilities.argmax().item()\n\n    return classes[predicted_class_index], probabilities[predicted_class_index].item()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:33:20.442064Z","iopub.execute_input":"2024-10-18T22:33:20.442442Z","iopub.status.idle":"2024-10-18T22:33:20.449095Z","shell.execute_reply.started":"2024-10-18T22:33:20.442406Z","shell.execute_reply":"2024-10-18T22:33:20.448064Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Cell 11: Live video classification\n\ncap = cv2.VideoCapture(2)\n\n\n\nwhile True:\n\n    ret, frame = cap.read()\n\n    if not ret:\n\n        break\n\n\n\n    class_name, confidence = predict(frame)\n\n    freshness = \"Fresh\" if \"Fresh\" in class_name else \"Rotten\"\n\n\n\n    cv2.putText(frame, f\"{class_name} ({freshness})\", (10, 30),\n\n                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    cv2.putText(frame, f\"Confidence: {confidence:.2f}\", (10, 60),\n\n                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n\n\n    cv2.imshow('Fruit Freshness Classifier', frame)\n\n\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n\n        break\n\n\n\ncap.release()\n\ncv2.destroyAllWindows()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-18T22:35:00.231101Z","iopub.execute_input":"2024-10-18T22:35:00.231866Z","iopub.status.idle":"2024-10-18T22:35:00.270263Z","shell.execute_reply.started":"2024-10-18T22:35:00.231827Z","shell.execute_reply":"2024-10-18T22:35:00.269039Z"}},"outputs":[{"name":"stderr","text":"[ WARN:0@1401.515] global cap_v4l.cpp:999 open VIDEOIO(V4L2:/dev/video2): can't open camera by index\n[ERROR:0@1401.515] global obsensor_uvc_stream_channel.cpp:158 getStreamChannelGroup Camera index out of range\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 45\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     43\u001b[0m cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m---> 45\u001b[0m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdestroyAllWindows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/highgui/src/window.cpp:1295: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n"],"ename":"error","evalue":"OpenCV(4.10.0) /io/opencv/modules/highgui/src/window.cpp:1295: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n","output_type":"error"}],"execution_count":17}]}